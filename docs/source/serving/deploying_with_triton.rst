.. _deploying_with_triton:

Deploying with NVIDIA Triton
============================

Visit `Triton Inference Server <https://github.com/triton-inference-server>`_ to learn how to quickly deploy a simple `facebook/opt-125m <https://huggingface.co/facebook/opt-125m>`_ model, using vLLM. See `Deploying a vLLM model in Triton <https://github.com/triton-inference-server/tutorials/blob/main/Quick_Deploy/vLLM/README.md#deploying-a-vllm-model-in-triton>`_ for more details.
